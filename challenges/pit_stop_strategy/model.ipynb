{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['raceId', 'season', 'raceNumber', 'circuitId', 'prixName', 'raceDate',\n",
       "       'driverId', 'constructorId', 'driverStartGridPos', 'driverFinalGridPos',\n",
       "       'driverFinalRank', 'driverRacePoints', 'driverLapCount',\n",
       "       'driverFatestLapNumber', 'driverFastestLapTime',\n",
       "       'driverFastestLapSpeed', 'constructorName', 'constructorNationality',\n",
       "       'constructorChampionshipStandingPoints',\n",
       "       'constructorChampionshipStandingPosition',\n",
       "       'constructorChampionshipStandingWins', 'constructorRacePoints',\n",
       "       'driverDateOfBirth', 'driverNationality',\n",
       "       'driverChampionshipStandingPoints',\n",
       "       'driverChampionshipStandingPosition', 'driverChampionshipStandingWins',\n",
       "       'circuitName', 'circuitLocation', 'circuitCountry', 'lat', 'lng', 'alt',\n",
       "       'driverRaceResultStatus', 'driverName', 'driverAge', 'race_time',\n",
       "       'driverRaceLapNumber', 'driverRaceFinalPosition', 'driverLapTime',\n",
       "       'driverLapTimeInMilliseconds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "races_data = pd.read_csv('../../../data/processed/fully_integrated_data.csv')\n",
    "lap_time_data = pd.read_csv(\"../../../data/processed/lap_times.csv\")\n",
    "\n",
    "# Merge datasets\n",
    "merged_data = pd.merge(\n",
    "    races_data,  # Changed from race_data to races_data\n",
    "    lap_time_data,\n",
    "    on=[\"raceId\", \"driverId\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "if \"Unnamed: 0\" in merged_data.columns:\n",
    "    merged_data = merged_data.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "merged_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeshw\\AppData\\Local\\Temp\\ipykernel_14408\\3440964919.py:10: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  .fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_data[\"TireDegradation\"] = (\n",
    "    merged_data.groupby([\"raceId\", \"driverId\"])[\"driverLapTimeInMilliseconds\"]\n",
    "    .diff() / merged_data.groupby(\"raceId\")[\"driverLapTimeInMilliseconds\"].transform(\"mean\")\n",
    ")\n",
    "\n",
    "# Fill missing values with median\n",
    "merged_data[\"TireDegradation\"] = (\n",
    "    merged_data.groupby([\"raceId\", \"driverId\"])[\"driverLapTimeInMilliseconds\"]\n",
    "    .diff()\n",
    "    .fillna(method='bfill')  \n",
    ") / merged_data.groupby(\"raceId\")[\"driverLapTimeInMilliseconds\"].transform(\"mean\")\n",
    "merged_data[\"TireDegradation\"] = merged_data.groupby(\n",
    "    [\"circuitId\"]\n",
    ")[\"TireDegradation\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "#normalize TireDegradation\n",
    "merged_data[\"TireDegradation\"] = (\n",
    "    merged_data[\"TireDegradation\"] - merged_data[\"TireDegradation\"].mean()\n",
    ") / merged_data[\"TireDegradation\"].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeshw\\AppData\\Local\\Temp\\ipykernel_14408\\2535372296.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['tire_age'] = df.groupby(['raceId', 'driverId']).apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TireDegradation</th>\n",
       "      <th>current_lap_stage</th>\n",
       "      <th>tire_age</th>\n",
       "      <th>track_position</th>\n",
       "      <th>traffic_density</th>\n",
       "      <th>pit_stop_loss</th>\n",
       "      <th>raceId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TireDegradation  current_lap_stage  tire_age  track_position  \\\n",
       "0                0                  0         0               0   \n",
       "1                0                  0         0               0   \n",
       "2                0                  0         0               0   \n",
       "3                0                  0         0               0   \n",
       "4                0                  0         1               0   \n",
       "\n",
       "   traffic_density  pit_stop_loss  raceId  \n",
       "0                1              1       1  \n",
       "1                1              1       1  \n",
       "2                1              1       1  \n",
       "3                1              1       1  \n",
       "4                1              1       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_state_space(df):\n",
    "    \n",
    "    df['total_laps'] = df.groupby('raceId')['driverLapCount'].transform('max')\n",
    "    \n",
    "    # Create normalized lap percentage and bin using FIXED thresholds\n",
    "    df['lap_pct'] = df['driverRaceLapNumber'] / df['total_laps']\n",
    "    df['current_lap_stage'] = pd.cut(\n",
    "        df['lap_pct'],\n",
    "        bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        labels=[0, 1, 2, 3, 4],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Track position using final rank (ensure no NaN values)\n",
    "    df['track_position'] = pd.cut(\n",
    "        df['driverFinalRank'].fillna(20),  # Assume missing = backmarker\n",
    "        bins=[0, 3.9, 10.9, 20],  # Exclusive right edges\n",
    "        labels=[0, 1, 2]  # Leading, Midfield, Back\n",
    "    )\n",
    "    \n",
    "    # Tire age calculation with proper grouping\n",
    "    df['tire_age'] = df.groupby(['raceId', 'driverId']).cumcount() + 1\n",
    "    \n",
    "    # Identify pit stops using lap time spikes (customize threshold per circuit)\n",
    "    pit_stop_threshold = df.groupby('circuitId')['driverLapTimeInMilliseconds'].transform(\n",
    "        lambda x: x.quantile(0.95)\n",
    "    )\n",
    "    df['pit_flag'] = (df['driverLapTimeInMilliseconds'] > pit_stop_threshold).astype(int)\n",
    "    \n",
    "    \n",
    "    df['tire_age'] = df.groupby(['raceId', 'driverId']).apply(\n",
    "        lambda g: g['tire_age'].where(g['pit_flag'] == 0, 0).cumsum()\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    df['tire_age'] = pd.cut(\n",
    "        df['tire_age'],\n",
    "        bins=[-1, 10, 20, 30, 50],  \n",
    "        labels=[0, 1, 2, 3]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    avg_lap = df.groupby('raceId')['driverLapTimeInMilliseconds'].transform('mean')\n",
    "    std_lap = df.groupby('raceId')['driverLapTimeInMilliseconds'].transform('std')\n",
    "    df['traffic_z'] = (df['driverLapTimeInMilliseconds'] - avg_lap) / std_lap\n",
    "    df['traffic_density'] = pd.cut(\n",
    "        df['traffic_z'],\n",
    "        bins=[-np.inf, -1, 1, np.inf],\n",
    "        labels=[0, 1, 2]  # Low, Medium, High traffic\n",
    "    )\n",
    "    \n",
    "\n",
    "    df['pit_stop_loss'] = pd.qcut(\n",
    "        df['driverLapTimeInMilliseconds'],\n",
    "        q=[0, 0.3, 0.7, 1],\n",
    "        labels=[0, 1, 2]  # Low, Medium, High\n",
    "    )\n",
    "    \n",
    "    state_cols = [\n",
    "        'TireDegradation',\n",
    "        'current_lap_stage',\n",
    "        'tire_age',\n",
    "        'track_position',\n",
    "        'traffic_density',\n",
    "        'pit_stop_loss',\n",
    "        'raceId',\n",
    "    ]\n",
    "    \n",
    "    return df[state_cols].fillna(0).astype(int)\n",
    "\n",
    "state_df = create_state_space(merged_data)\n",
    "state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TireDegradation      0\n",
      "current_lap_stage    0\n",
      "tire_age             0\n",
      "track_position       0\n",
      "traffic_density      0\n",
      "pit_stop_loss        0\n",
      "raceId               0\n",
      "dtype: int64\n",
      "       TireDegradation  current_lap_stage       tire_age  track_position  \\\n",
      "count    589081.000000      589081.000000  589081.000000   589081.000000   \n",
      "mean          0.000716           1.903015       0.196937        1.237981   \n",
      "std           0.969342           1.405782       0.666225        0.743334   \n",
      "min         -49.000000           0.000000       0.000000        0.000000   \n",
      "25%           0.000000           1.000000       0.000000        1.000000   \n",
      "50%           0.000000           2.000000       0.000000        1.000000   \n",
      "75%           0.000000           3.000000       0.000000        2.000000   \n",
      "max          49.000000           4.000000       3.000000        2.000000   \n",
      "\n",
      "       traffic_density  pit_stop_loss         raceId  \n",
      "count    589081.000000  589081.000000  589081.000000  \n",
      "mean          1.070591       0.999956     600.544465  \n",
      "std           0.272717       0.774619     434.375976  \n",
      "min           0.000000       0.000000       1.000000  \n",
      "25%           1.000000       0.000000     140.000000  \n",
      "50%           1.000000       1.000000     861.000000  \n",
      "75%           1.000000       2.000000    1003.000000  \n",
      "max           2.000000       2.000000    1144.000000  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['TireDegradation', 'current_lap_stage', 'tire_age', 'track_position',\n",
       "       'traffic_density', 'pit_stop_loss', 'raceId'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(state_df.isnull().sum())\n",
    "print(state_df.describe(include='all'))\n",
    "\n",
    "state_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\n",
    "    \"PIT_NEXT_LAP\", \n",
    "    \"PIT_IN_2_LAPS\", \n",
    "    \"PIT_IN_3_LAPS\", \n",
    "    \"PIT_IN_4_LAPS\", \n",
    "    \"PIT_IN_5_LAPS\", \n",
    "    \"NO_PIT\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100/1000, Total Reward: -28.55, Epsilon: 0.61\n",
      "Episode: 200/1000, Total Reward: -12.65, Epsilon: 0.37\n",
      "Episode: 300/1000, Total Reward: -0.35, Epsilon: 0.22\n",
      "Episode: 400/1000, Total Reward: 1.30, Epsilon: 0.13\n",
      "Episode: 500/1000, Total Reward: -4.05, Epsilon: 0.08\n",
      "Episode: 600/1000, Total Reward: 6.40, Epsilon: 0.05\n",
      "Episode: 700/1000, Total Reward: 6.25, Epsilon: 0.03\n",
      "Episode: 800/1000, Total Reward: 9.25, Epsilon: 0.02\n",
      "Episode: 900/1000, Total Reward: 8.10, Epsilon: 0.01\n",
      "Episode: 1000/1000, Total Reward: 9.90, Epsilon: 0.01\n",
      "Training complete. Model saved to pit_stop_strategy_model.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Configure logging; set level to WARNING and save logs to file.\n",
    "logging.basicConfig(level=logging.WARNING, filename='pit_stop_strategy.log')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ==============================\n",
    "# Environment for Pit Stop Strategy\n",
    "# ==============================\n",
    "class PitStopEnv(gym.Env):\n",
    "    def __init__(self, max_laps=100):\n",
    "        super(PitStopEnv, self).__init__()\n",
    "        self.max_laps = max_laps\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(7,), dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.actions = [\n",
    "            \"PIT_NEXT_LAP\", \n",
    "            \"PIT_IN_2_LAPS\", \n",
    "            \"PIT_IN_3_LAPS\", \n",
    "            \"PIT_IN_4_LAPS\", \n",
    "            \"PIT_IN_5_LAPS\", \n",
    "            \"NO_PIT\"\n",
    "        ]\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.tire_degradation = 0.0         # Tire degradation starts at 0.\n",
    "        self.current_lap_stage = 1          # Start at lap stage 1.\n",
    "        self.tire_age = 0.0                 # No tire age at the beginning.\n",
    "        self.track_position = 0             # Starting track position (can be modified as needed).\n",
    "        self.traffic_density = 0.0          # Assume no traffic density initially.\n",
    "        self.pit_stop_loss = 0.0            # No pit stop loss at the start.\n",
    "        self.raceId = 1                     # Set raceId to 1 for the episode.\n",
    "        \n",
    "        self.state = np.array([\n",
    "            self.tire_degradation, \n",
    "            self.current_lap_stage, \n",
    "            self.tire_age, \n",
    "            self.track_position, \n",
    "            self.traffic_density, \n",
    "            self.pit_stop_loss, \n",
    "            float(self.raceId)\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        self.total_reward = 0.0\n",
    "        self.done = False\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        action_name = self.actions[action]\n",
    "        logger.info(f\"Current state: {self.state}\")\n",
    "        reward = 0.0\n",
    "        \n",
    "        # If NO_PIT, give a small progress reward and update tire degradation and age.\n",
    "        if action_name == \"NO_PIT\":\n",
    "            reward += 0.1\n",
    "            logger.info(\"Action: NO_PIT, rewarding progress: +0.1\")\n",
    "            self.tire_age += 1.0\n",
    "            # Simulate gradual degradation when not pitting.\n",
    "            self.tire_degradation += np.random.uniform(0.1, 0.3)\n",
    "            self.pit_stop_loss = 0.0  # No pit loss when not pitting.\n",
    "        else:\n",
    "            # For pit actions, determine the pit delay.\n",
    "            if action_name == \"PIT_NEXT_LAP\":\n",
    "                pit_delay = 1\n",
    "            else:\n",
    "                pit_delay = int(action_name.split(\"_\")[2])\n",
    "            # Apply a penalty that increases slightly with delay.\n",
    "            penalty = -0.5 - (pit_delay - 1) * 0.05\n",
    "            reward += penalty\n",
    "            logger.info(f\"Action: {action_name}, applying penalty: {penalty}\")\n",
    "            # Reset tire age and degradation upon pitting.\n",
    "            self.tire_age = 0.0\n",
    "            self.tire_degradation = 0.0\n",
    "            # Set pit_stop_loss proportional to pit delay.\n",
    "            self.pit_stop_loss = pit_delay * 1.0\n",
    "        \n",
    "        # Update current lap stage.\n",
    "        self.current_lap_stage += 1\n",
    "        \n",
    "        # Update track_position (simulate slight random change).\n",
    "        self.track_position += np.random.choice([-1, 0, 1])\n",
    "        # Update traffic_density randomly between 0 and 5.\n",
    "        self.traffic_density = np.random.uniform(0, 5)\n",
    "        \n",
    "        # RaceId remains constant during an episode.\n",
    "        new_state = np.array([\n",
    "            self.tire_degradation, \n",
    "            self.current_lap_stage, \n",
    "            self.tire_age, \n",
    "            self.track_position, \n",
    "            self.traffic_density, \n",
    "            self.pit_stop_loss, \n",
    "            float(self.raceId)\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        if abs(new_state[1] - self.state[1]) > 100:\n",
    "            logger.warning(f\"Drastic lap stage change: {self.state[1]} -> {new_state[1]}\")\n",
    "        \n",
    "        self.state = new_state\n",
    "        \n",
    "        # End the episode if current lap stage reaches max_laps.\n",
    "        if self.current_lap_stage >= self.max_laps:\n",
    "            self.done = True\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        \n",
    "        info = {\"total_reward\": self.total_reward}\n",
    "        return self.state, reward, self.done, info\n",
    "\n",
    "# ==============================\n",
    "# DQN Model in PyTorch\n",
    "# ==============================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# ==============================\n",
    "# DQN Agent Implementation in PyTorch\n",
    "# ==============================\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = DQN(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            act_values = self.model(state_tensor)\n",
    "        return torch.argmax(act_values[0]).item()\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                with torch.no_grad():\n",
    "                    next_q = self.model(next_state_tensor)\n",
    "                target = reward + self.gamma * torch.max(next_q).item()\n",
    "            current_q = self.model(state_tensor)\n",
    "            target_f = current_q.clone().detach()\n",
    "            target_f[0][action] = target\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(state_tensor)\n",
    "            loss = self.criterion(output, target_f)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# ==============================\n",
    "# Training Loop in PyTorch\n",
    "# ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = PitStopEnv(max_laps=100)\n",
    "state_size = env.observation_space.shape[0]  # now 7\n",
    "action_size = env.action_space.n             # 6 actions\n",
    "agent = DQNAgent(state_size, action_size, device)\n",
    "episodes = 1000\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_episode_reward = 0\n",
    "    for t in range(env.max_laps):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_episode_reward += reward\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "    if (e+1) % 100 == 0:\n",
    "        print(f\"Episode: {e+1}/{episodes}, Total Reward: {total_episode_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "model_save_path = \"pit_stop_strategy_model.pth\"\n",
    "torch.save(agent.model.state_dict(), model_save_path)\n",
    "print(f\"Training complete. Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *Total Reward Trend:*\n",
    "   - *Initial Episodes (1-100):* The total reward is highly negative (-28.50), indicating the agent is performing poorly. This is expected in the early stages of training, as the agent is exploring randomly (high ε).\n",
    "   - *Mid-Training (200-400):* The reward improves significantly, moving from -12.05 to 1.35. This suggests the agent is learning better strategies.\n",
    "   - *Later Episodes (500-1000):* The reward stabilizes around 7-10, indicating the agent has converged to a reasonably good policy.\n",
    "\n",
    "2. *Epsilon Decay:*\n",
    "   - *Epsilon (ε):* Starts at 0.61 (high exploration) and decays to 0.01 (low exploration) by the end of training. This is typical for ε-greedy exploration, where the agent shifts from exploration to exploitation as it learns.\n",
    "\n",
    "3. *Final Performance:*\n",
    "   - The final reward (9.90) is positive, which is a good sign. It means the agent is achieving its objectives (e.g., minimizing pit stops, maximizing lap efficiency).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
