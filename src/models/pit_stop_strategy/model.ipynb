{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['raceId', 'season', 'raceNumber', 'circuitId', 'prixName', 'raceDate',\n",
       "       'driverId', 'constructorId', 'driverStartGridPos', 'driverFinalGridPos',\n",
       "       'driverFinalRank', 'driverRacePoints', 'driverLapCount',\n",
       "       'driverFatestLapNumber', 'driverFastestLapTime',\n",
       "       'driverFastestLapSpeed', 'constructorName', 'constructorNationality',\n",
       "       'constructorChampionshipStandingPoints',\n",
       "       'constructorChampionshipStandingPosition',\n",
       "       'constructorChampionshipStandingWins', 'constructorRacePoints',\n",
       "       'driverDateOfBirth', 'driverNationality',\n",
       "       'driverChampionshipStandingPoints',\n",
       "       'driverChampionshipStandingPosition', 'driverChampionshipStandingWins',\n",
       "       'circuitName', 'circuitLocation', 'circuitCountry', 'lat', 'lng', 'alt',\n",
       "       'driverRaceResultStatus', 'driverName', 'driverAge', 'race_time',\n",
       "       'driverRaceLapNumber', 'driverRaceFinalPosition', 'driverLapTime',\n",
       "       'driverLapTimeInMilliseconds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "races_data = pd.read_csv('../../../data/processed/fully_integrated_data.csv')\n",
    "lap_time_data = pd.read_csv(\"../../../data/processed/lap_times.csv\")\n",
    "\n",
    "# Merge datasets\n",
    "merged_data = pd.merge(\n",
    "    races_data,  # Changed from race_data to races_data\n",
    "    lap_time_data,\n",
    "    on=[\"raceId\", \"driverId\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "if \"Unnamed: 0\" in merged_data.columns:\n",
    "    merged_data = merged_data.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "merged_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeshw\\AppData\\Local\\Temp\\ipykernel_2456\\3440964919.py:10: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  .fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_data[\"TireDegradation\"] = (\n",
    "    merged_data.groupby([\"raceId\", \"driverId\"])[\"driverLapTimeInMilliseconds\"]\n",
    "    .diff() / merged_data.groupby(\"raceId\")[\"driverLapTimeInMilliseconds\"].transform(\"mean\")\n",
    ")\n",
    "\n",
    "# Fill missing values with median\n",
    "merged_data[\"TireDegradation\"] = (\n",
    "    merged_data.groupby([\"raceId\", \"driverId\"])[\"driverLapTimeInMilliseconds\"]\n",
    "    .diff()\n",
    "    .fillna(method='bfill')  \n",
    ") / merged_data.groupby(\"raceId\")[\"driverLapTimeInMilliseconds\"].transform(\"mean\")\n",
    "merged_data[\"TireDegradation\"] = merged_data.groupby(\n",
    "    [\"circuitId\"]\n",
    ")[\"TireDegradation\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "#normalize TireDegradation\n",
    "merged_data[\"TireDegradation\"] = (\n",
    "    merged_data[\"TireDegradation\"] - merged_data[\"TireDegradation\"].mean()\n",
    ") / merged_data[\"TireDegradation\"].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeshw\\AppData\\Local\\Temp\\ipykernel_2456\\2535372296.py:31: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['tire_age'] = df.groupby(['raceId', 'driverId']).apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TireDegradation</th>\n",
       "      <th>current_lap_stage</th>\n",
       "      <th>tire_age</th>\n",
       "      <th>track_position</th>\n",
       "      <th>traffic_density</th>\n",
       "      <th>pit_stop_loss</th>\n",
       "      <th>raceId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TireDegradation  current_lap_stage  tire_age  track_position  \\\n",
       "0                0                  0         0               0   \n",
       "1                0                  0         0               0   \n",
       "2                0                  0         0               0   \n",
       "3                0                  0         0               0   \n",
       "4                0                  0         1               0   \n",
       "\n",
       "   traffic_density  pit_stop_loss  raceId  \n",
       "0                1              1       1  \n",
       "1                1              1       1  \n",
       "2                1              1       1  \n",
       "3                1              1       1  \n",
       "4                1              1       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_state_space(df):\n",
    "    \n",
    "    df['total_laps'] = df.groupby('raceId')['driverLapCount'].transform('max')\n",
    "    \n",
    "    # Create normalized lap percentage and bin using FIXED thresholds\n",
    "    df['lap_pct'] = df['driverRaceLapNumber'] / df['total_laps']\n",
    "    df['current_lap_stage'] = pd.cut(\n",
    "        df['lap_pct'],\n",
    "        bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        labels=[0, 1, 2, 3, 4],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Track position using final rank (ensure no NaN values)\n",
    "    df['track_position'] = pd.cut(\n",
    "        df['driverFinalRank'].fillna(20),  # Assume missing = backmarker\n",
    "        bins=[0, 3.9, 10.9, 20],  # Exclusive right edges\n",
    "        labels=[0, 1, 2]  # Leading, Midfield, Back\n",
    "    )\n",
    "    \n",
    "    # Tire age calculation with proper grouping\n",
    "    df['tire_age'] = df.groupby(['raceId', 'driverId']).cumcount() + 1\n",
    "    \n",
    "    # Identify pit stops using lap time spikes (customize threshold per circuit)\n",
    "    pit_stop_threshold = df.groupby('circuitId')['driverLapTimeInMilliseconds'].transform(\n",
    "        lambda x: x.quantile(0.95)\n",
    "    )\n",
    "    df['pit_flag'] = (df['driverLapTimeInMilliseconds'] > pit_stop_threshold).astype(int)\n",
    "    \n",
    "    \n",
    "    df['tire_age'] = df.groupby(['raceId', 'driverId']).apply(\n",
    "        lambda g: g['tire_age'].where(g['pit_flag'] == 0, 0).cumsum()\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    df['tire_age'] = pd.cut(\n",
    "        df['tire_age'],\n",
    "        bins=[-1, 10, 20, 30, 50],  \n",
    "        labels=[0, 1, 2, 3]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    avg_lap = df.groupby('raceId')['driverLapTimeInMilliseconds'].transform('mean')\n",
    "    std_lap = df.groupby('raceId')['driverLapTimeInMilliseconds'].transform('std')\n",
    "    df['traffic_z'] = (df['driverLapTimeInMilliseconds'] - avg_lap) / std_lap\n",
    "    df['traffic_density'] = pd.cut(\n",
    "        df['traffic_z'],\n",
    "        bins=[-np.inf, -1, 1, np.inf],\n",
    "        labels=[0, 1, 2]  # Low, Medium, High traffic\n",
    "    )\n",
    "    \n",
    "\n",
    "    df['pit_stop_loss'] = pd.qcut(\n",
    "        df['driverLapTimeInMilliseconds'],\n",
    "        q=[0, 0.3, 0.7, 1],\n",
    "        labels=[0, 1, 2]  # Low, Medium, High\n",
    "    )\n",
    "    \n",
    "    state_cols = [\n",
    "        'TireDegradation',\n",
    "        'current_lap_stage',\n",
    "        'tire_age',\n",
    "        'track_position',\n",
    "        'traffic_density',\n",
    "        'pit_stop_loss',\n",
    "        'raceId',\n",
    "    ]\n",
    "    \n",
    "    return df[state_cols].fillna(0).astype(int)\n",
    "\n",
    "state_df = create_state_space(merged_data)\n",
    "state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TireDegradation      0\n",
      "current_lap_stage    0\n",
      "tire_age             0\n",
      "track_position       0\n",
      "traffic_density      0\n",
      "pit_stop_loss        0\n",
      "raceId               0\n",
      "dtype: int64\n",
      "       TireDegradation  current_lap_stage       tire_age  track_position  \\\n",
      "count    589081.000000      589081.000000  589081.000000   589081.000000   \n",
      "mean          0.000716           1.903015       0.196937        1.237981   \n",
      "std           0.969342           1.405782       0.666225        0.743334   \n",
      "min         -49.000000           0.000000       0.000000        0.000000   \n",
      "25%           0.000000           1.000000       0.000000        1.000000   \n",
      "50%           0.000000           2.000000       0.000000        1.000000   \n",
      "75%           0.000000           3.000000       0.000000        2.000000   \n",
      "max          49.000000           4.000000       3.000000        2.000000   \n",
      "\n",
      "       traffic_density  pit_stop_loss         raceId  \n",
      "count    589081.000000  589081.000000  589081.000000  \n",
      "mean          1.070591       0.999956     600.544465  \n",
      "std           0.272717       0.774619     434.375976  \n",
      "min           0.000000       0.000000       1.000000  \n",
      "25%           1.000000       0.000000     140.000000  \n",
      "50%           1.000000       1.000000     861.000000  \n",
      "75%           1.000000       2.000000    1003.000000  \n",
      "max           2.000000       2.000000    1144.000000  \n"
     ]
    }
   ],
   "source": [
    "print(state_df.isnull().sum())\n",
    "print(state_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\n",
    "    \"PIT_NEXT_LAP\", \n",
    "    \"PIT_IN_2_LAPS\", \n",
    "    \"PIT_IN_3_LAPS\", \n",
    "    \"PIT_IN_4_LAPS\", \n",
    "    \"PIT_IN_5_LAPS\", \n",
    "    \"NO_PIT\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pit stop time penalties in milliseconds\n",
    "pit_stop_loss = {\n",
    "    'Low': 20000,     # 20 seconds\n",
    "    'Medium': 25000,  # 25 seconds\n",
    "    'High': 30000     # 30 seconds\n",
    "}\n",
    "\n",
    "def update_state(state, action):\n",
    "    # Convert list state to dictionary for easier handling\n",
    "    state_dict = {\n",
    "        \"current_lap_stage\": state[0],\n",
    "        \"tire_age\": state[1],\n",
    "        \"track_position\": state[2],\n",
    "        \"traffic_density\": state[3],\n",
    "        \"pit_stop_loss\": state[4]\n",
    "    }\n",
    "    new_state = state_dict.copy()\n",
    "    \n",
    "    # Update tire age (within bounds 0-3)\n",
    "    new_state[\"tire_age\"] = min(3, new_state[\"tire_age\"] + 1)\n",
    "    \n",
    "    # Apply pit stop consequences\n",
    "    if action != \"NO_PIT\":\n",
    "        pit_lap = int(action.split(\"_\")[-2])  # Extract lap number from action\n",
    "        new_state[\"current_lap_stage\"] = 0  # Reset lap stage after pit\n",
    "        if state_dict[\"current_lap_stage\"] >= pit_lap:\n",
    "            new_state[\"tire_age\"] = 0  # Fresh tires\n",
    "            # Lose positions based on traffic density\n",
    "            if state_dict[\"traffic_density\"] == 2:  # High traffic\n",
    "                new_state[\"track_position\"] = min(3, new_state[\"track_position\"] + 1)\n",
    "    \n",
    "    # Convert back to list format matching state_dims order\n",
    "    return [\n",
    "        new_state[\"current_lap_stage\"],\n",
    "        new_state[\"tire_age\"],\n",
    "        new_state[\"track_position\"],\n",
    "        new_state[\"traffic_density\"],\n",
    "        new_state[\"pit_stop_loss\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class F1PitStrategyEnv(gym.Env):\n",
    "    def __init__(self, state_df):\n",
    "        super(F1PitStrategyEnv, self).__init__()\n",
    "        \n",
    "        # Ensure state_df has the required columns\n",
    "        required_columns = ['raceId', 'current_lap_stage', 'tire_age', 'track_position', 'traffic_density', 'pit_stop_loss']\n",
    "        if not all(col in state_df.columns for col in required_columns):\n",
    "            raise ValueError(f\"state_df must contain the following columns: {required_columns}\")\n",
    "        \n",
    "        self.state_df = state_df\n",
    "        self.races = self.state_df['raceId'].unique()\n",
    "        self.current_race_idx = 0\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # State space dimensions\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0, 0]),  # Min values for each feature\n",
    "            high=np.array([\n",
    "                self.state_df['current_lap_stage'].max(),\n",
    "                self.state_df['tire_age'].max(),\n",
    "                self.state_df['track_position'].max(),\n",
    "                self.state_df['traffic_density'].max(),\n",
    "                self.state_df['pit_stop_loss'].max()\n",
    "            ]),\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        \n",
    "        # Action space: 6 discrete options\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        \n",
    "    def reset(self):\n",
    "        # Get initial state for a new race\n",
    "        self.current_race = self.races[self.current_race_idx]\n",
    "        race_data = self.state_df[self.state_df['raceId'] == self.current_race]\n",
    "        self.race_steps = race_data.drop(columns=['raceId']).values  # Exclude raceId from state\n",
    "        self.current_step = 0\n",
    "        self.current_race_idx = (self.current_race_idx + 1) % len(self.races)\n",
    "        return self.race_steps[self.current_step]\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Get current state\n",
    "        state = self.race_steps[self.current_step]\n",
    "        \n",
    "        # Execute action (simple version)\n",
    "        new_state = self.race_steps[min(self.current_step + 1, len(self.race_steps)-1)]\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._calculate_reward(state, new_state, action)\n",
    "        \n",
    "        # Update step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.race_steps)\n",
    "        \n",
    "        return new_state, reward, done, {}\n",
    "    \n",
    "    def _calculate_reward(self, state, new_state, action):\n",
    "        state = np.array(state, dtype=int)\n",
    "        new_state = np.array(new_state, dtype=int)\n",
    "        \n",
    "        # Simplified reward function\n",
    "        position_improvement = float(state[2] - new_state[2])  # Lower position number is better\n",
    "        tire_penalty = float(-0.1 * state[1])  # Older tires get penalty\n",
    "        pit_penalty = -1.0 if action < 5 else 0.0  # Less discouraging\n",
    "        \n",
    "        return position_improvement + tire_penalty + pit_penalty\n",
    "    # Hypothetical reward function adjustments\n",
    "def get_reward(state, action):\n",
    "    base_reward = -0.1  # Small penalty per step to encourage efficiency\n",
    "    \n",
    "    if action == \"PIT_NEXT_LAP\":\n",
    "        # Penalize pit time but less severely\n",
    "        return base_reward - 0.5\n",
    "    elif action == \"NO_PIT\":\n",
    "        # Penalize high tire wear\n",
    "        tire_wear = state[2]  # Assuming index 2 tracks tire wear\n",
    "        return base_reward - (tire_wear * 0.1)\n",
    "    # Reward for lap completion\n",
    "    if state[0] % 1 == 0:  # After completing a lap\n",
    "        return base_reward + 1.0\n",
    "    return base_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -1023.9\n",
      "Episode 100, Total Reward: -1148.099999999999\n",
      "Episode 200, Total Reward: -727.6000000000014\n",
      "Episode 300, Total Reward: -917.3000000000002\n",
      "Episode 400, Total Reward: -1027.2000000000005\n",
      "Episode 500, Total Reward: -613.6000000000006\n",
      "Episode 600, Total Reward: -516.100000000001\n",
      "Episode 700, Total Reward: -441.3000000000011\n",
      "Episode 800, Total Reward: -570.3000000000008\n",
      "Episode 900, Total Reward: -329.19999999999993\n"
     ]
    }
   ],
   "source": [
    "# Initialize Q-table\n",
    "state_dims = [\n",
    "    int(state_df['current_lap_stage'].max() + 1),\n",
    "    int(state_df['tire_age'].max() + 1),\n",
    "    int(state_df['track_position'].max() + 1),\n",
    "    int(state_df['traffic_density'].max() + 1),\n",
    "    int(state_df['pit_stop_loss'].max() + 1),\n",
    "]\n",
    "num_actions = 6\n",
    "q_table = np.zeros(shape=tuple(state_dims + [num_actions]))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "n_episodes = 1000\n",
    "\n",
    "# Initialize environment\n",
    "env = F1PitStrategyEnv(state_df)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    # During training loop (replace fixed epsilon)\n",
    "    epsilon = max(0.01, 1.0 - episode / n_episodes)  # Decays from 1.0 to 0.01\n",
    "    \n",
    "    while not done:\n",
    "        # Clip state values to valid range (only first 5 components)\n",
    "        state = np.array(state[:5])  # Take only the first 5 components\n",
    "        state = np.clip(state, 0, np.array(state_dims) - 1)\n",
    "        \n",
    "        # Convert state to a tuple of integers\n",
    "        state_tuple = tuple(map(int, state))\n",
    "        \n",
    "        # Validate state tuple\n",
    "        assert all(0 <= x < dim for x, dim in zip(state_tuple, state_dims)), f\"Invalid state: {state_tuple}\"\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.randint(num_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state_tuple])\n",
    "        \n",
    "        # Execute action\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Clip new_state values to valid range\n",
    "        new_state = np.array(state[:5]) \n",
    "        new_state = np.clip(new_state, 0, np.array(state_dims) - 1)\n",
    "        \n",
    "        # Convert new_state to a tuple of integers\n",
    "        new_state_tuple = tuple(map(int, new_state))\n",
    "        \n",
    "        # Validate new_state tuple\n",
    "        assert all(0 <= x < dim for x, dim in zip(new_state_tuple, state_dims)), f\"Invalid new_state: {new_state_tuple}\"\n",
    "        \n",
    "        # Q-table update\n",
    "        old_q = q_table[state_tuple][action]\n",
    "        max_future_q = np.max(q_table[new_state_tuple])\n",
    "        new_q = (1 - alpha) * old_q + alpha * (reward + gamma * max_future_q)\n",
    "        q_table[state_tuple][action] = new_q\n",
    "        \n",
    "        # Update state and total reward\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "        \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 1], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 1 0 1 1], Action: NO_PIT, Reward: -1.0\n",
      "Current state: [0 0 2 0 1 1], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 0 1 1], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 0 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 0 1 1], Action: PIT_NEXT_LAP, Reward: 2.0\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 2 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 2 0 0 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [20  2  0  0  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-21   2   0   0   1   1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 0 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 1], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 1 0 1 1], Action: NO_PIT, Reward: -1.0\n",
      "Current state: [0 0 2 0 1 1], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 0 1 1], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 0 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 0 1 1], Action: PIT_NEXT_LAP, Reward: 2.0\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 2 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 2 0 0 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [1 2 0 0 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [18  2  0  0  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-17   2   0   0   1   2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-1  2  0  0  1  1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 0 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 0 0 1 2], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 0 1 0 1 1], Action: NO_PIT, Reward: -1.0\n",
      "Current state: [0 0 2 0 1 1], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 0 1 1], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 0 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 0 1 1], Action: PIT_NEXT_LAP, Reward: 2.0\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: 0.0\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 1 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 2 0 0 1 1], Action: NO_PIT, Reward: -0.1\n",
      "Current state: [0 2 0 0 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [20  2  0  0  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-20   2   0   0   1   1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 0 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 0 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 3 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.30000000000000004\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 4 0 0 1 0], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 0 0 1 1 2], Action: NO_PIT, Reward: -0.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 1 1 1 1], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 2 1 1 1], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 1 1 1], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: 2.0\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [20  2  0  1  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-21   2   0   1   1   1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 1 1 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 2 1 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 1 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: 2.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [20  2  0  1  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-20   2   0   1   1   1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 1 1 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 2 1 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 1 1 1], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: 2.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [20  2  0  1  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-20   2   0   1   1   1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 1 1 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 2 1 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 1 1 1], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: 2.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [18  2  0  1  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-17   2   0   1   1   2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 1 1 1 1], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 2 2 1 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 1 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: 1.7000000000000002\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [18  2  0  1  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-17   2   0   1   1   2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-1  2  0  1  1  1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 1 1 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 2 2 1 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 1 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: 1.7000000000000002\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [18  2  0  1  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-18   2   0   1   1   2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 1 1 1 1], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 2 2 1 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 1 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: 1.7000000000000002\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [18  2  0  1  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-18   2   0   1   1   2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 1 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 1 1 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 2 1 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 1 1 0], Action: PIT_NEXT_LAP, Reward: -2.3\n",
      "Current state: [0 3 3 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: 1.7000000000000002\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 1 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [20  2  0  2  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-20   2   0   2   1   1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 1 2 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 2 2 2 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 2 1 2], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: 1.7000000000000002\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [18  2  0  2  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-17   2   0   2   1   2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 1 2 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 2 2 2 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 2 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: 1.7000000000000002\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [18  2  0  2  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-17   2   0   2   1   2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-1  2  1  2  1  1], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 2 2 2 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 2 3 2 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: 1.7000000000000002\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [18  2  0  2  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-18   2   0   2   1   2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-1  2  1  2  1  1], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 2 2 2 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 2 3 2 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: 1.7000000000000002\n",
      "Current state: [0 3 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [18  2  0  2  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-17   2   0   2   1   2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 1 2 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 2 2 1 0], Action: PIT_NEXT_LAP, Reward: -2.2\n",
      "Current state: [0 3 3 2 1 0], Action: PIT_NEXT_LAP, Reward: -2.3\n",
      "Current state: [0 3 3 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: 1.7000000000000002\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 1 2 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 2 2 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 2 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: 2.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [18  2  0  2  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-18   2   0   2   1   2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 1 2 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 2 2 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 2 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 3 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 2], Action: PIT_NEXT_LAP, Reward: 2.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [18  2  0  2  2  2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [-18   2   0   2   1   2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 3 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.3\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 0], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 4 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.4\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 1 2 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 2 2 1 2], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 2 1 1], Action: PIT_NEXT_LAP, Reward: -2.0\n",
      "Current state: [0 0 3 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: 2.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 0 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.0\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 1 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.1\n",
      "Current state: [0 2 0 2 1 2], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Current state: [0 2 0 2 1 1], Action: PIT_NEXT_LAP, Reward: -1.2\n",
      "Episode finished with total reward: -1202.3999999999985\n"
     ]
    }
   ],
   "source": [
    "def test_agent(env, q_table):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Clip state values to valid range\n",
    "        state = np.clip(state[:5], 0, np.array(state_dims) - 1)\n",
    "        state_tuple = tuple(map(int, state))\n",
    "        \n",
    "        # Get action from Q-table\n",
    "        action = np.argmax(q_table[state_tuple])\n",
    "        \n",
    "        # Take step in environment\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        print(f\"Current state: {state}, Action: {actions[action]}, Reward: {reward}\")\n",
    "    \n",
    "    print(f\"Episode finished with total reward: {total_reward}\")\n",
    "\n",
    "test_agent(env, q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100/1000, Total Reward: -29.90, Epsilon: 0.61\n",
      "Episode: 200/1000, Total Reward: -12.05, Epsilon: 0.37\n",
      "Episode: 300/1000, Total Reward: -8.20, Epsilon: 0.22\n",
      "Episode: 400/1000, Total Reward: 4.35, Epsilon: 0.13\n",
      "Episode: 500/1000, Total Reward: 4.20, Epsilon: 0.08\n",
      "Episode: 600/1000, Total Reward: 9.10, Epsilon: 0.05\n",
      "Episode: 700/1000, Total Reward: 7.70, Epsilon: 0.03\n",
      "Episode: 800/1000, Total Reward: 8.35, Epsilon: 0.02\n",
      "Episode: 900/1000, Total Reward: 9.20, Epsilon: 0.01\n",
      "Episode: 1000/1000, Total Reward: 9.90, Epsilon: 0.01\n",
      "Training complete. Model saved to pit_stop_strategy_model.pth\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Configure logging for debugging; set to WARNING so that INFO messages are not printed\n",
    "logging.basicConfig(level=logging.WARNING, filename='pit_stop_strategy.log')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ==============================\n",
    "# Environment for Pit Stop Strategy\n",
    "# ==============================\n",
    "class PitStopEnv(gym.Env):\n",
    "    def __init__(self, max_laps=100):\n",
    "        super(PitStopEnv, self).__init__()\n",
    "        self.max_laps = max_laps\n",
    "        \n",
    "        # Define state: [current_lap, tire_age, position, gap_to_leader, traffic_density, extra_feature]\n",
    "        self.state_dtype = np.int32  \n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "        \n",
    "        # Define action space with 6 actions:\n",
    "        # 0 = PIT_NEXT_LAP, 1 = PIT_IN_2_LAPS, 2 = PIT_IN_3_LAPS, 3 = PIT_IN_4_LAPS, 4 = PIT_IN_5_LAPS, 5 = NO_PIT\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.actions = [\n",
    "            \"PIT_NEXT_LAP\", \n",
    "            \"PIT_IN_2_LAPS\", \n",
    "            \"PIT_IN_3_LAPS\", \n",
    "            \"PIT_IN_4_LAPS\", \n",
    "            \"PIT_IN_5_LAPS\", \n",
    "            \"NO_PIT\"\n",
    "        ]\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_lap = np.int32(1)\n",
    "        self.tire_age = 0.0\n",
    "        self.position = 0\n",
    "        self.gap_to_leader = 0.0\n",
    "        self.traffic_density = 0.0\n",
    "        self.extra_feature = 0.0\n",
    "        self.state = np.array([self.current_lap, self.tire_age, self.position,\n",
    "                               self.gap_to_leader, self.traffic_density, self.extra_feature],\n",
    "                              dtype=np.float32)\n",
    "        self.total_reward = 0.0\n",
    "        self.done = False\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        action_name = self.actions[action]\n",
    "        # (Logging is now set to WARNING; INFO messages will not print to console)\n",
    "        logger.info(f\"Current state: {self.state}\")\n",
    "        reward = 0.0\n",
    "        \n",
    "        if action_name == \"NO_PIT\":\n",
    "            reward += 0.1\n",
    "            logger.info(\"Action: NO_PIT, rewarding progress: +0.1\")\n",
    "            self.tire_age += 1.0\n",
    "        else:\n",
    "            if action_name == \"PIT_NEXT_LAP\":\n",
    "                pit_delay = 1\n",
    "            else:\n",
    "                pit_delay = int(action_name.split(\"_\")[2])\n",
    "            penalty = -0.5 - (pit_delay - 1) * 0.05\n",
    "            reward += penalty\n",
    "            logger.info(f\"Action: {action_name}, applying penalty: {penalty}\")\n",
    "            self.tire_age = 0.0\n",
    "        \n",
    "        new_lap = self.current_lap + 1\n",
    "        if new_lap < self.current_lap:\n",
    "            logger.warning(f\"Lap counter anomaly: {self.current_lap} -> {new_lap}\")\n",
    "        self.current_lap = np.int32(new_lap)\n",
    "        \n",
    "        self.position += np.random.choice([-1, 0, 1])\n",
    "        self.gap_to_leader += np.random.uniform(-0.5, 0.5)\n",
    "        self.traffic_density = np.random.uniform(0, 5)\n",
    "        self.extra_feature = np.random.uniform(0, 1)\n",
    "        \n",
    "        new_state = np.array([self.current_lap, self.tire_age, self.position,\n",
    "                              self.gap_to_leader, self.traffic_density, self.extra_feature],\n",
    "                             dtype=np.float32)\n",
    "        \n",
    "        if abs(new_state[0] - self.state[0]) > 100:\n",
    "            logger.warning(f\"Drastic lap counter change: {self.state[0]} -> {new_state[0]}\")\n",
    "        \n",
    "        self.state = new_state\n",
    "        \n",
    "        if self.current_lap >= self.max_laps:\n",
    "            self.done = True\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        \n",
    "        info = {\"total_reward\": self.total_reward}\n",
    "        return self.state, reward, self.done, info\n",
    "\n",
    "# ==============================\n",
    "# DQN Model in PyTorch\n",
    "# ==============================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# ==============================\n",
    "# DQN Agent Implementation in PyTorch\n",
    "# ==============================\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = device\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = DQN(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            act_values = self.model(state_tensor)\n",
    "        return torch.argmax(act_values[0]).item()\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                with torch.no_grad():\n",
    "                    next_q = self.model(next_state_tensor)\n",
    "                target = reward + self.gamma * torch.max(next_q).item()\n",
    "            current_q = self.model(state_tensor)\n",
    "            target_f = current_q.clone().detach()\n",
    "            target_f[0][action] = target\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(state_tensor)\n",
    "            loss = self.criterion(output, target_f)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# ==============================\n",
    "# Training Loop in PyTorch\n",
    "# ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = PitStopEnv(max_laps=100)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size, device)\n",
    "episodes = 1000\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_episode_reward = 0\n",
    "    for t in range(env.max_laps):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_episode_reward += reward\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "    # Only final summary is printed via logger at WARNING level or higher\n",
    "    if (e+1) % 100 == 0:\n",
    "        print(f\"Episode: {e+1}/{episodes}, Total Reward: {total_episode_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "# Save the trained model to disk\n",
    "model_save_path = \"pit_stop_strategy_model.pth\"\n",
    "torch.save(agent.model.state_dict(), model_save_path)\n",
    "print(f\"Training complete. Model saved to {model_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
